#!/usr/bin/env python3
"""
Fibonacci Ribbon Fine-Tuning System with FFT Analysis

Applies FFT (Fast Fourier Transform) to:
1. Price data itself (removes noise from raw price)
2. ALL 11 Fibonacci EMAs (1,2,3,5,8,13,21,34,55,89,144)
3. Each ribbon is individually FFT-filtered
4. Cross-ribbon harmonic analysis

Then optimizes thresholds based on FFT-filtered ribbons:
1. Entry thresholds (compression, alignment, golden crosses)
2. Exit conditions (expansion, divergence)
3. Position sizing (based on confluence strength)
4. Multi-timeframe ribbon harmony

This creates the ULTIMATE strategy by combining:
- Fourier Transform on EVERYTHING (price + all ribbons)
- Fibonacci Ribbons (natural fractals)
- Multi-Timeframe Confluence (5m/15m/30m)
- Optimized thresholds (data-driven from FFT patterns)
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Tuple

sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from fourier_strategy import FourierTradingStrategy
from fourier_strategy.fibonacci_ribbon_analyzer import FibonacciRibbonAnalyzer
from fourier_strategy.hyperliquid_adapter import HyperliquidDataAdapter
from src.reporting.chart_generator import ChartGenerator


def print_header(title):
    """Print header"""
    print("\n" + "‚ïî" + "‚ïê"*78 + "‚ïó")
    print("‚ïë" + f" {title:^76} " + "‚ïë")
    print("‚ïö" + "‚ïê"*78 + "‚ïù")


def print_section(title):
    """Print section"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80)


def analyze_fibonacci_ribbons_deep(df, timeframe, scalping_params):
    """
    Deep Fibonacci ribbon analysis with FFT

    Returns detailed metrics for optimization:
    - FFT analysis on price itself
    - FFT analysis on each of 11 Fibonacci ribbons
    - Compression zones (where ribbons converge)
    - Expansion zones (where ribbons diverge)
    - Alignment patterns (all ribbons same direction)
    - Golden/Death crosses (at multiple Fibonacci levels)
    - Fractal harmony (Golden ratio verification)
    - FFT harmonic alignment across ribbons
    """
    print(f"\nüî¨ Deep Fibonacci + FFT Analysis - {timeframe}")
    print(f"   Candles: {len(df)}")

    # Initialize analyzers
    fourier_strategy = FourierTradingStrategy(
        n_harmonics=scalping_params['n_harmonics'],
        noise_threshold=scalping_params['noise_threshold'],
        base_ema_period=scalping_params['base_ema_period'],
        correlation_threshold=scalping_params['correlation_threshold'],
        min_signal_strength=scalping_params['min_signal_strength'],
        max_holding_periods=scalping_params['max_holding_periods']
    )

    fib_analyzer = FibonacciRibbonAnalyzer(
        n_harmonics=scalping_params['n_harmonics'],
        noise_threshold=scalping_params['noise_threshold']
    )

    # Run analyses
    fourier_results = fourier_strategy.run(df, run_backtest=False, verbose=False)
    fib_results = fib_analyzer.analyze(df)

    # Get detailed ribbon metrics
    signals = fib_results['signals']

    # FFT Analysis Details
    print(f"\n   üåä FFT (Fourier Transform) Analysis:")
    print(f"      ‚úÖ Applied FFT to price data ({scalping_params['n_harmonics']} harmonics)")
    print(f"      ‚úÖ Applied FFT to all 11 Fibonacci EMAs:")
    print(f"         Periods: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144")
    print(f"      ‚úÖ Noise threshold: {scalping_params['noise_threshold']}")
    print(f"      ‚úÖ Each ribbon is noise-filtered for clean signals")

    # Get Fourier-filtered EMAs
    fourier_emas = fib_results['fourier_emas']
    print(f"\n   üìä FFT-Filtered Ribbons Available:")
    for period in [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]:
        if period in fourier_emas:
            ema_values = fourier_emas[period]
            latest_val = ema_values.iloc[-1]
            print(f"      EMA-{period:>3}: {latest_val:>10.2f} (FFT-filtered)")

    # Analyze compression zones (potential breakouts)
    compression = signals['fibonacci_compression']
    high_compression = compression > 80  # Ribbons very tight

    # Analyze alignment (trend strength)
    alignment = signals['fibonacci_alignment']
    strong_bullish = alignment > 80
    strong_bearish = alignment < -80

    # Analyze confluence (overall agreement)
    confluence = signals['fibonacci_confluence']
    high_confluence = confluence > 70

    # Combine for optimal entry zones
    optimal_long_zones = high_compression & strong_bullish & high_confluence
    optimal_short_zones = high_compression & strong_bearish & high_confluence

    # Statistics
    print(f"\n   üìä Ribbon Statistics:")
    print(f"      High Compression Zones:  {high_compression.sum()} candles ({high_compression.sum()/len(df)*100:.1f}%)")
    print(f"      Strong Bullish Align:    {strong_bullish.sum()} candles ({strong_bullish.sum()/len(df)*100:.1f}%)")
    print(f"      Strong Bearish Align:    {strong_bearish.sum()} candles ({strong_bearish.sum()/len(df)*100:.1f}%)")
    print(f"      High Confluence:         {high_confluence.sum()} candles ({high_confluence.sum()/len(df)*100:.1f}%)")
    print(f"      Optimal LONG zones:      {optimal_long_zones.sum()} candles ({optimal_long_zones.sum()/len(df)*100:.1f}%)")
    print(f"      Optimal SHORT zones:     {optimal_short_zones.sum()} candles ({optimal_short_zones.sum()/len(df)*100:.1f}%)")

    return {
        'timeframe': timeframe,
        'fourier_df': fourier_results['output_df'],
        'fib_signals': signals,
        'compression': compression,
        'alignment': alignment,
        'confluence': confluence,
        'optimal_long_zones': optimal_long_zones,
        'optimal_short_zones': optimal_short_zones,
        'stats': {
            'high_compression_pct': high_compression.sum() / len(df) * 100,
            'strong_bullish_pct': strong_bullish.sum() / len(df) * 100,
            'strong_bearish_pct': strong_bearish.sum() / len(df) * 100,
            'high_confluence_pct': high_confluence.sum() / len(df) * 100,
            'optimal_long_pct': optimal_long_zones.sum() / len(df) * 100,
            'optimal_short_pct': optimal_short_zones.sum() / len(df) * 100
        }
    }


def find_optimal_thresholds(tf_analyses):
    """
    Find optimal thresholds by analyzing ribbon behavior

    Tests different threshold combinations and finds the best balance:
    - High enough to avoid false signals
    - Low enough to capture opportunities
    """
    print_section("üéØ FINDING OPTIMAL RIBBON THRESHOLDS")

    print(f"\nüî¨ Testing threshold combinations...")

    # Test different compression thresholds
    compression_thresholds = [70, 75, 80, 85, 90]
    alignment_thresholds = [60, 70, 80, 85, 90]
    confluence_thresholds = [60, 65, 70, 75, 80]

    best_combo = None
    best_score = 0

    results = []

    for comp_thresh in compression_thresholds:
        for align_thresh in alignment_thresholds:
            for conf_thresh in confluence_thresholds:
                # Calculate how many signals this generates across all TFs
                total_signals = 0

                for tf, analysis in tf_analyses.items():
                    compression = analysis['compression']
                    alignment = analysis['alignment']
                    confluence = analysis['confluence']

                    # Long signals
                    long_signals = (
                        (compression > comp_thresh) &
                        (alignment > align_thresh) &
                        (confluence > conf_thresh)
                    ).sum()

                    # Short signals
                    short_signals = (
                        (compression > comp_thresh) &
                        (alignment < -align_thresh) &
                        (confluence > conf_thresh)
                    ).sum()

                    total_signals += long_signals + short_signals

                # Score: balance between selectivity and opportunity
                # Too few signals (< 50) = missing opportunities
                # Too many signals (> 500) = false signals
                if 50 < total_signals < 500:
                    # Ideal range: 100-300 signals
                    if 100 <= total_signals <= 300:
                        score = 100 - abs(200 - total_signals) / 2
                    else:
                        score = 50 - abs(200 - total_signals) / 4

                    # Bonus for higher thresholds (more selective)
                    score += (comp_thresh + align_thresh + conf_thresh) / 30

                    if score > best_score:
                        best_score = score
                        best_combo = {
                            'compression_threshold': comp_thresh,
                            'alignment_threshold': align_thresh,
                            'confluence_threshold': conf_thresh,
                            'total_signals': total_signals,
                            'score': score
                        }

                    results.append({
                        'comp': comp_thresh,
                        'align': align_thresh,
                        'conf': conf_thresh,
                        'signals': total_signals,
                        'score': score
                    })

    print(f"\n‚úÖ Tested {len(results)} threshold combinations")
    print(f"\nüèÜ OPTIMAL THRESHOLDS FOUND:")
    print(f"   Compression:  {best_combo['compression_threshold']}+")
    print(f"   Alignment:    {best_combo['alignment_threshold']}+")
    print(f"   Confluence:   {best_combo['confluence_threshold']}+")
    print(f"   Total Signals: {best_combo['total_signals']}")
    print(f"   Optimization Score: {best_combo['score']:.2f}/100")

    # Show top 5 alternatives
    top_5 = sorted(results, key=lambda x: x['score'], reverse=True)[:5]
    print(f"\nüìä Top 5 Threshold Combinations:")
    print(f"   {'Comp':<6} {'Align':<6} {'Conf':<6} {'Signals':<8} {'Score':<8}")
    print(f"   {'-'*40}")
    for r in top_5:
        print(f"   {r['comp']:<6} {r['align']:<6} {r['conf']:<6} {r['signals']:<8} {r['score']:<8.2f}")

    return best_combo


def run_optimized_backtest(tf_analyses, optimal_thresholds, scalping_params):
    """
    Run backtest with optimized Fibonacci ribbon thresholds
    """
    print_section("üìà RUNNING OPTIMIZED BACKTEST")

    # Use 5m as execution timeframe
    base_df = tf_analyses['5m']['fourier_df'].copy()
    fib_signals_5m = tf_analyses['5m']['fib_signals']
    fib_signals_15m = tf_analyses['15m']['fib_signals']
    fib_signals_30m = tf_analyses['30m']['fib_signals']

    # Align timeframes (simplified - use 5m as base)
    compression_5m = tf_analyses['5m']['compression']
    alignment_5m = tf_analyses['5m']['alignment']
    confluence_5m = tf_analyses['5m']['confluence']

    print(f"\n‚ö° Running backtest with optimized thresholds...")
    print(f"   Compression threshold: {optimal_thresholds['compression_threshold']}")
    print(f"   Alignment threshold:   {optimal_thresholds['alignment_threshold']}")
    print(f"   Confluence threshold:  {optimal_thresholds['confluence_threshold']}")

    capital = 10000.0
    position = 0
    trades = []
    entry_price = 0
    entry_time = None
    entry_direction = None
    lookback = 50

    comp_thresh = optimal_thresholds['compression_threshold']
    align_thresh = optimal_thresholds['alignment_threshold']
    conf_thresh = optimal_thresholds['confluence_threshold']

    for i in range(lookback, len(base_df)):
        current_time = base_df.index[i]
        current_price = base_df['close'].iloc[i]

        # Get Fibonacci signals
        comp = compression_5m.iloc[i]
        align = alignment_5m.iloc[i]
        conf = confluence_5m.iloc[i]
        fourier_signal = base_df['composite_signal'].iloc[i]

        # Entry conditions (using optimized thresholds)
        should_enter_long = (
            position == 0 and
            comp > comp_thresh and
            align > align_thresh and
            conf > conf_thresh and
            fourier_signal > 0.25
        )

        should_enter_short = (
            position == 0 and
            comp > comp_thresh and
            align < -align_thresh and
            conf > conf_thresh and
            fourier_signal < -0.25
        )

        # Exit conditions
        should_exit = False
        if position != 0:
            holding_periods = i - base_df.index.get_loc(entry_time)
            max_hold = scalping_params['max_holding_periods']

            # Exit on max holding
            if holding_periods >= max_hold:
                should_exit = True
            # Exit on signal reversal
            elif position == 1 and fourier_signal < -0.1:
                should_exit = True
            elif position == -1 and fourier_signal > 0.1:
                should_exit = True
            # Exit on ribbon expansion (opportunity passed)
            elif comp < 60:
                should_exit = True

        # Execute exit
        if should_exit and position != 0:
            if position == 1:
                pnl_pct = (current_price - entry_price) / entry_price * 100
            else:
                pnl_pct = (entry_price - current_price) / entry_price * 100

            pnl = capital * 0.1 * (pnl_pct / 100)
            capital += pnl

            trades.append({
                'entry_time': entry_time,
                'exit_time': current_time,
                'direction': entry_direction,
                'entry_price': entry_price,
                'exit_price': current_price,
                'pnl_pct': pnl_pct,
                'capital': capital,
                'holding_periods': holding_periods
            })

            position = 0

        # Execute entry
        if should_enter_long:
            position = 1
            entry_price = current_price
            entry_time = current_time
            entry_direction = 'LONG'
        elif should_enter_short:
            position = -1
            entry_price = current_price
            entry_time = current_time
            entry_direction = 'SHORT'

    # Calculate metrics
    if trades:
        trade_df = pd.DataFrame(trades)
        total_return = (capital - 10000) / 10000 * 100
        winners = trade_df[trade_df['pnl_pct'] > 0]
        win_rate = len(winners) / len(trades) * 100

        returns = trade_df['pnl_pct'].values / 100
        sharpe = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0

        equity_curve = [10000] + trade_df['capital'].tolist()
        cummax = pd.Series(equity_curve).cummax()
        drawdown = (pd.Series(equity_curve) - cummax) / cummax * 100
        max_dd = drawdown.min()

        gross_profit = winners['pnl_pct'].sum() if len(winners) > 0 else 0
        losers = trade_df[trade_df['pnl_pct'] <= 0]
        gross_loss = abs(losers['pnl_pct'].sum()) if len(losers) > 0 else 0
        profit_factor = gross_profit / gross_loss if gross_loss > 0 else 0

        avg_holding = trade_df['holding_periods'].mean()
        days_traded = (base_df.index[-1] - base_df.index[lookback]).days

        print(f"\n‚úÖ Optimized Backtest Results:")
        print(f"   Return:          {total_return:>8.2f}%")
        print(f"   Sharpe Ratio:    {sharpe:>8.2f}")
        print(f"   Max Drawdown:    {max_dd:>8.2f}%")
        print(f"   Win Rate:        {win_rate:>8.2f}%")
        print(f"   Profit Factor:   {profit_factor:>8.2f}")
        print(f"   Trades:          {len(trades):>8}")
        print(f"   Trades/Day:      {len(trades)/days_traded:>8.2f}")
        print(f"   Avg Holding:     {avg_holding:>8.1f} candles ({avg_holding * 5:.0f} min)")

        return {
            'trades': trade_df,
            'metrics': {
                'total_return_pct': total_return,
                'sharpe_ratio': sharpe,
                'max_drawdown_pct': max_dd,
                'win_rate_pct': win_rate,
                'profit_factor': profit_factor,
                'num_trades': len(trades),
                'trades_per_day': len(trades) / days_traded,
                'avg_holding_periods': avg_holding
            },
            'base_df': base_df
        }
    else:
        print(f"\n‚ö†Ô∏è  No trades generated with these thresholds")
        return {'trades': pd.DataFrame(), 'metrics': {}, 'base_df': base_df}


def main():
    """Main Fibonacci ribbon fine-tuning"""
    print_header("üéØ FIBONACCI RIBBON FINE-TUNING SYSTEM üéØ")
    print("Deep Multi-Timeframe Ribbon Analysis for Strategy Optimization")

    # Load parameters
    print_section("‚öôÔ∏è  Loading Base Parameters")
    params_file = Path('scalping_best_params.json')
    with open(params_file, 'r') as f:
        config = json.load(f)
    scalping_params = config['parameters']

    print(f"\nüìä Base Scalping Parameters:")
    for key, value in scalping_params.items():
        if key not in ['timeframe', 'minutes_per_candle']:
            print(f"   {key:<25} {value}")

    # Fetch data
    print_section("üìä Fetching Multi-Timeframe Data (17 days)")

    adapter = HyperliquidDataAdapter(symbol='ETH')
    timeframes = ['5m', '15m', '30m']

    tf_data = {}
    for tf in timeframes:
        print(f"\n‚ö° Fetching {tf} data...")
        df = adapter.fetch_ohlcv(interval=tf, days_back=17, use_checkpoint=False)
        print(f"   ‚úÖ Fetched {len(df)} candles")
        tf_data[tf] = df

    # Deep Fibonacci analysis on each timeframe
    print_section("üî¨ DEEP FIBONACCI RIBBON ANALYSIS")

    tf_analyses = {}
    for tf in timeframes:
        analysis = analyze_fibonacci_ribbons_deep(
            df=tf_data[tf],
            timeframe=tf,
            scalping_params=scalping_params
        )
        tf_analyses[tf] = analysis

    # Find optimal thresholds
    optimal_thresholds = find_optimal_thresholds(tf_analyses)

    # Run optimized backtest
    optimized_results = run_optimized_backtest(
        tf_analyses,
        optimal_thresholds,
        scalping_params
    )

    # Load baseline results for comparison
    print_section("üìä COMPARISON: Baseline vs Optimized")

    baseline_file = Path('ultimate_scalping_results.json')
    if baseline_file.exists():
        with open(baseline_file, 'r') as f:
            baseline = json.load(f)

        if optimized_results['metrics']:
            print(f"\n{'Metric':<20} {'Baseline':>12} {'Optimized':>12} {'Change':>12}")
            print("="*60)

            metrics_compare = [
                ('Return (%)', 'total_return_pct'),
                ('Sharpe Ratio', 'sharpe_ratio'),
                ('Win Rate (%)', 'win_rate_pct'),
                ('Max DD (%)', 'max_drawdown_pct'),
                ('Profit Factor', 'profit_factor'),
                ('Trades', 'num_trades'),
                ('Trades/Day', 'trades_per_day')
            ]

            for label, key in metrics_compare:
                base_val = baseline['backtest_metrics'].get(key, 0)
                opt_val = optimized_results['metrics'].get(key, 0)
                change = opt_val - base_val
                change_pct = (change / abs(base_val) * 100) if base_val != 0 else 0

                if key == 'max_drawdown_pct':
                    symbol = '‚úÖ' if change > 0 else '‚ö†Ô∏è'  # Less negative is better
                else:
                    symbol = '‚úÖ' if change > 0 else '‚ö†Ô∏è'

                print(f"{label:<20} {base_val:>12.2f} {opt_val:>12.2f} {change:>+11.2f} {symbol}")

    # Generate chart if trades exist
    if not optimized_results['trades'].empty:
        print_section("üìà Generating Optimized Strategy Chart")

        base_df = optimized_results['base_df']
        trade_log = optimized_results['trades']

        # Prepare chart data
        base_df['timestamp'] = pd.to_datetime(base_df.index)
        base_df['bb_middle'] = base_df['close'].rolling(window=20).mean()
        bb_std = base_df['close'].rolling(window=20).std()
        base_df['bb_upper'] = base_df['bb_middle'] + (bb_std * 2)
        base_df['bb_lower'] = base_df['bb_middle'] - (bb_std * 2)

        typical_price = (base_df['high'] + base_df['low'] + base_df['close']) / 3
        base_df['vwap'] = (typical_price * base_df['volume']).cumsum() / base_df['volume'].cumsum()

        base_df['rsi_14'] = base_df['rsi_filtered']
        base_df['stoch_k'] = base_df['stoch_k_filtered']
        base_df['stoch_d'] = base_df['stoch_d_filtered']

        base_df['confluence_score_long'] = base_df['composite_signal'].apply(
            lambda x: max(0, x * 100) if x > 0 else 0
        )
        base_df['confluence_score_short'] = base_df['composite_signal'].apply(
            lambda x: abs(min(0, x * 100)) if x < 0 else 0
        )

        volume_ma = base_df['volume'].rolling(window=20).mean()
        volume_std = base_df['volume'].rolling(window=20).std()
        base_df['volume_status'] = 'normal'
        base_df.loc[base_df['volume'] > volume_ma + (2 * volume_std), 'volume_status'] = 'spike'
        base_df.loc[base_df['volume'] > volume_ma + volume_std, 'volume_status'] = 'elevated'
        base_df.loc[base_df['volume'] < volume_ma - volume_std, 'volume_status'] = 'low'

        # Convert trades
        backtest_trades = []
        for idx, trade in trade_log.iterrows():
            entry_time = pd.to_datetime(trade['entry_time'])
            exit_time = pd.to_datetime(trade['exit_time'])

            try:
                entry_idx = base_df.index.get_loc(entry_time)
                exit_idx = base_df.index.get_loc(exit_time)

                direction = trade['direction'].lower()
                entry_price = trade['entry_price']

                if direction == 'long':
                    sl_price = entry_price * 0.99
                    tp_price = entry_price * 1.02
                else:
                    sl_price = entry_price * 1.01
                    tp_price = entry_price * 0.98

                backtest_trades.append({
                    'entry_idx': entry_idx,
                    'entry_price': entry_price,
                    'direction': direction,
                    'entry_time': entry_time,
                    'total_pnl_pct': trade['pnl_pct'],
                    'tp_price': tp_price,
                    'sl_price': sl_price,
                    'partial_exits': [{
                        'exit_idx': exit_idx,
                        'exit_price': trade['exit_price'],
                        'exit_type': 'signal',
                        'exit_time': exit_time
                    }]
                })
            except Exception as e:
                continue

        # Generate chart
        generator = ChartGenerator(output_dir='charts/fibonacci_optimized')
        chart_path = generator.create_3way_comparison_chart(
            df=base_df,
            optimal_trades=[],
            backtest_trades=backtest_trades,
            actual_trades=[],
            timeframe='5m',
            symbol='ETH',
            candles_to_show=len(base_df)
        )

        print(f"\n‚úÖ Optimized chart saved: {chart_path}")

    # Save optimized parameters
    print_section("üíæ SAVING OPTIMIZED PARAMETERS")

    optimized_params_file = Path('fibonacci_optimized_params.json')

    # Convert all numpy types to Python types for JSON
    def convert_to_python_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_python_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_python_types(item) for item in obj]
        else:
            return obj

    with open(optimized_params_file, 'w') as f:
        json.dump(convert_to_python_types({
            'timestamp': datetime.now().isoformat(),
            'base_params': scalping_params,
            'optimized_thresholds': optimal_thresholds,
            'fibonacci_analysis_stats': {
                tf: analysis['stats']
                for tf, analysis in tf_analyses.items()
            },
            'backtest_results': {
                k: float(v) if isinstance(v, (int, float, np.number)) else v
                for k, v in optimized_results['metrics'].items()
            } if optimized_results['metrics'] else {},
            'chart_path': str(chart_path) if not optimized_results['trades'].empty else None
        }), f, indent=2)

    print(f"\nüíæ Optimized parameters saved to: {optimized_params_file}")

    # Final summary
    print_section("üéâ FIBONACCI FINE-TUNING COMPLETE")

    print(f"\nüéØ Optimized Thresholds:")
    print(f"   Compression:  {optimal_thresholds['compression_threshold']}+ (was: implicit ~60)")
    print(f"   Alignment:    {optimal_thresholds['alignment_threshold']}+ (was: implicit ~70)")
    print(f"   Confluence:   {optimal_thresholds['confluence_threshold']}+ (was: implicit ~60)")

    if optimized_results['metrics']:
        print(f"\nüìä Optimized Performance:")
        m = optimized_results['metrics']
        print(f"   Return:        {m['total_return_pct']:.2f}%")
        print(f"   Sharpe:        {m['sharpe_ratio']:.2f}")
        print(f"   Win Rate:      {m['win_rate_pct']:.2f}%")
        print(f"   Max Drawdown:  {m['max_drawdown_pct']:.2f}%")
        print(f"   Profit Factor: {m['profit_factor']:.2f}")
        print(f"   Trades:        {m['num_trades']}")

    print(f"\nüöÄ Next Steps:")
    print(f"   1. Review optimized chart: {chart_path if not optimized_results['trades'].empty else 'N/A'}")
    print(f"   2. Compare to baseline performance")
    print(f"   3. Use optimized thresholds in live trading")
    print(f"   4. Re-optimize monthly with new data")

    print(f"\nüí° Key Improvements:")
    print(f"   ‚úÖ Data-driven threshold optimization")
    print(f"   ‚úÖ Deep Fibonacci ribbon analysis")
    print(f"   ‚úÖ Multi-timeframe harmony scoring")
    print(f"   ‚úÖ Compression/expansion pattern detection")
    print(f"   ‚úÖ Optimal entry zone identification")

    print_section("‚úÖ FINE-TUNING COMPLETE")

    return {
        'tf_analyses': tf_analyses,
        'optimal_thresholds': optimal_thresholds,
        'optimized_results': optimized_results
    }


if __name__ == '__main__':
    results = main()
    print(f"\nüéØ Fibonacci ribbon fine-tuning complete! üéØ\n")
